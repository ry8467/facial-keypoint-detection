{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Facial Keypoint Detection Demo"
      ],
      "metadata": {
        "id": "5QpQgcquUT5V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8348vFErMZLD"
      },
      "source": [
        "**Recommending runing in Google Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2AMyPZnMSdF"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/your-username/your-repo/blob/main/notebooks/your-notebook.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5ulIGJCc1lS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision import models, transforms, datasets\n",
        "from torchvision.io import read_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load and Visualize Data"
      ],
      "metadata": {
        "id": "HNsuVfS_UW0S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiF6Jknuc6JI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83589308-812e-4c34-8114-01ffb14d7d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./data/test.zip\n",
            "  inflating: test.csv                \n"
          ]
        }
      ],
      "source": [
        "!unzip ./data/test.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kIC7G72dC4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d21e72-031e-4490-d01e-52e172c73cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./data/training.zip\n",
            "  inflating: training.csv            \n"
          ]
        }
      ],
      "source": [
        "!unzip ./data/training.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLctbv-pdlJh"
      },
      "outputs": [],
      "source": [
        "class FacialKeypointsDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        keypoint_columns = [\n",
        "            'left_eye_center_x', 'left_eye_center_y',\n",
        "            'right_eye_center_x', 'right_eye_center_y',\n",
        "            'left_eye_inner_corner_x', 'left_eye_inner_corner_y',\n",
        "            'left_eye_outer_corner_x', 'left_eye_outer_corner_y',\n",
        "            'right_eye_inner_corner_x', 'right_eye_inner_corner_y',\n",
        "            'right_eye_outer_corner_x', 'right_eye_outer_corner_y',\n",
        "            'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y',\n",
        "            'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y',\n",
        "            'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y',\n",
        "            'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y',\n",
        "            'nose_tip_x', 'nose_tip_y',\n",
        "            'mouth_left_corner_x', 'mouth_left_corner_y',\n",
        "            'mouth_right_corner_x', 'mouth_right_corner_y',\n",
        "            'mouth_center_top_lip_x', 'mouth_center_top_lip_y',\n",
        "            'mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y'\n",
        "        ]\n",
        "        self.keypoint_columns = [col for col in keypoint_columns if col in self.annotations.columns]\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, f'{idx}.png')\n",
        "        if not os.path.exists(img_name):\n",
        "             return None\n",
        "\n",
        "        image = Image.open(img_name).convert(\"L\")\n",
        "\n",
        "        label = self.annotations.loc[idx, self.keypoint_columns].values.astype('float32')\n",
        "\n",
        "        label[np.isnan(label)] = 0.0\n",
        "\n",
        "        label = torch.tensor(label)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX_JYBc3d3Ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27785904-67e0-4202-d0a0-8bcaea482b72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 7049 images in the 'generated_images' directory.\n"
          ]
        }
      ],
      "source": [
        "output_dir = 'generated_images'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "train_df = pd.read_csv('./training.csv')\n",
        "\n",
        "for index, row in train_df.iterrows():\n",
        "    image_pixels = row['Image'].split(' ')\n",
        "    image_array = np.array([p for p in image_pixels if p != ''], dtype=np.uint8)\n",
        "\n",
        "    try:\n",
        "        img = Image.fromarray(image_array.reshape(96, 96), mode='L')\n",
        "        filename = f'{output_dir}/{index}.png'\n",
        "        img.save(filename)\n",
        "    except ValueError as e:\n",
        "        print(f\"Skipping row {index} due to incorrect pixel data length: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Generated {len(train_df)} images in the '{output_dir}' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Dataset and Transforms"
      ],
      "metadata": {
        "id": "afg4lKpUUasQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAt7zRwX2taQ"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnL0FcrF2ye3"
      },
      "outputs": [],
      "source": [
        "csv_file='training.csv'\n",
        "root_dir='generated_images'\n",
        "dataset = FacialKeypointsDataset(csv_file, root_dir, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW7Gt81SE35f"
      },
      "outputs": [],
      "source": [
        "valid_indices = [i for i in range(len(dataset)) if dataset[i] is not None]\n",
        "valid_dataset = torch.utils.data.Subset(dataset, valid_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyLvaogfE6Ba"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(valid_dataset))\n",
        "val_size = len(valid_dataset) - train_size\n",
        "train_set, val_set = random_split(valid_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPhPn0x02_XP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946eadda-503d-4fe1-a289-52d70674bcee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_set, batch_size=128, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build the Model"
      ],
      "metadata": {
        "id": "xy5C6K84UeQj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cADwxHUc3Dz7"
      },
      "outputs": [],
      "source": [
        "class KeypointCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(KeypointCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 10 * 10, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 30)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train the Model"
      ],
      "metadata": {
        "id": "-IboL2NdUhfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyCYbR0f6ahH"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')\n",
        "model = KeypointCNN().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHV2jMM7Gxd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304c89c5-84ce-487c-a2f7-eded74426d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Training Loss: 525.8964, Validation Loss: 410.8529\n",
            "Epoch 2/5, Training Loss: 462.7041, Validation Loss: 392.2003\n",
            "Epoch 3/5, Training Loss: 434.3959, Validation Loss: 369.1794\n",
            "Epoch 4/5, Training Loss: 394.4913, Validation Loss: 329.7998\n",
            "Epoch 5/5, Training Loss: 333.1174, Validation Loss: 315.1005\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "best_val_loss = float('inf')\n",
        "epoch = 0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    loss = running_loss / len(train_loader)\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluate on Validation Set"
      ],
      "metadata": {
        "id": "vG101BEjUlT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'test'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "train_df = pd.read_csv('./test.csv')\n",
        "\n",
        "for index, row in train_df.iterrows():\n",
        "    image_pixels = row['Image'].split(' ')\n",
        "    image_array = np.array([p for p in image_pixels if p != ''], dtype=np.uint8)\n",
        "\n",
        "    try:\n",
        "        img = Image.fromarray(image_array.reshape(96, 96), mode='L')\n",
        "        filename = f'{output_dir}/{index}.png'\n",
        "        img.save(filename)\n",
        "    except ValueError as e:\n",
        "        print(f\"Skipping row {index} due to incorrect pixel data length: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"Generated {len(train_df)} images in the '{output_dir}' directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fke2dk8gwFlM",
        "outputId": "52a20038-01ec-4df8-f521-44d90b02a42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 1783 images in the 'test' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "9GhUHa3pPqje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KeypointCNN()\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "VHWMch2xMq1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb7beaf-17e2-4a6f-eb1c-e08293bfba38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KeypointCNN(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (9): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=25600, out_features=64, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "    (4): Linear(in_features=64, out_features=30, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "Kk7OyRiBNgc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, keypoints in dataloader:\n",
        "            images, keypoints = images.to(device), keypoints.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, keypoints.view(outputs.shape))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "N3uEz1eRMuwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss = evaluate_model(model, val_loader, criterion, device)\n",
        "print(f\"Validation MSE: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "bj-HW58pNarI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e746186-9f69-4099-fca4-803629651f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MSE: 315.1005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Predict on Test Images"
      ],
      "metadata": {
        "id": "XB4q21nYUn_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_display(model, image_path, device):\n",
        "    image = Image.open(image_path).convert(\"L\")\n",
        "    transformed = transform(image).unsqueeze(0).to(device)\n",
        "    orig_w, orig_h = image.size\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(transformed)\n",
        "        keypoints = output.view(-1, 2).cpu().numpy()\n",
        "\n",
        "        keypoints = keypoints * 48 + 48\n",
        "\n",
        "        keypoints[:, 0] = keypoints[:, 0] * (orig_w / 96)\n",
        "        keypoints[:, 1] = keypoints[:, 1] * (orig_h / 96)\n",
        "\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.scatter(keypoints[:, 0], keypoints[:, 1], c='red', s=20)\n",
        "    plt.title(\"Predicted Keypoints on Original Image\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1aJnsN3hvaEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_and_display(model, '/content/test/0.png', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "BKtJM8Isv3Sm",
        "outputId": "e0f9daa2-9671-4e77-d16f-97d3f748dbd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}